{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4e6c4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "edfe5481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 03:59:58,378\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-10-13 03:59:58,388\tINFO client_builder.py:241 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n",
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUCCESS! Ray is connected using the FQDN.\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "\n",
    "# Use the FQDN to explicitly address the service in the 'default' namespace\n",
    "RAY_HEAD_ADDRESS = \"ray://raycluster-latest-head-svc.ray.svc.cluster.local:10001\"\n",
    "\n",
    "try:\n",
    "    # Run this code inside the Python environment of your remote kernel\n",
    "    ray.init(RAY_HEAD_ADDRESS)\n",
    "    print(\"SUCCESS! Ray is connected using the FQDN.\")\n",
    "    # You can now proceed to run Ray tasks\n",
    "except Exception as e:\n",
    "    print(f\"Connection still failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c0ae64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-13 04:17:58,285\tINFO client_builder.py:241 -- Passing the following kwargs to ray.init() on the server: log_to_driver\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SIGTERM handler is not set because current thread is not the main thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray connection established.\n",
      "Submitting GPU task...\n",
      "\n",
      "--- GPU Task Result ---\n",
      "Task executed on node: raycluster-latest-gpu-worker-group-worker-ll57f\n",
      "CUDA Available (GPU Found by PyTorch): True\n",
      "CUDA Devices Found: 1\n"
     ]
    }
   ],
   "source": [
    "import ray\n",
    "import time\n",
    "import torch # We'll use torch to prove the environment is GPU-ready\n",
    "\n",
    "# 1. Ensure Ray is connected (you ran this successfully before)\n",
    "# Use the FQDN to connect to the Ray Head Service in the 'default' namespace\n",
    "RAY_HEAD_ADDRESS = \"ray://raycluster-latest-head-svc.ray.svc.cluster.local:10001\"\n",
    "\n",
    "if not ray.is_initialized():\n",
    "    try:\n",
    "        ray.init(RAY_HEAD_ADDRESS, runtime_env={\"pip\": [\"torch\", \"torchvision\", \"torchaudio\"]})\n",
    "        print(\"Ray connection established.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Connection Failed: {e}\")\n",
    "        # If connection fails, check Kubernetes service name or firewall.\n",
    "\n",
    "# 2. Define a remote function that explicitly requests one GPU\n",
    "@ray.remote(num_gpus=1)\n",
    "def check_gpu_status():\n",
    "    \"\"\"\n",
    "    A remote Ray task that runs on a GPU worker and checks for PyTorch's CUDA availability.\n",
    "    \"\"\"\n",
    "    import socket\n",
    "    \n",
    "    # Check if a GPU is visible to this worker process\n",
    "    gpu_available = torch.cuda.is_available()\n",
    "    \n",
    "    # Get the worker node's hostname (i.e., the Kubernetes Pod name)\n",
    "    worker_hostname = socket.gethostname()\n",
    "    \n",
    "    return {\n",
    "        \"hostname\": worker_hostname,\n",
    "        \"cuda_available\": gpu_available,\n",
    "        \"device_count\": torch.cuda.device_count()\n",
    "    }\n",
    "\n",
    "# 3. Execute the task and retrieve the result\n",
    "print(\"Submitting GPU task...\")\n",
    "\n",
    "# Submit the task to the cluster (it will wait for a GPU worker to be available)\n",
    "future = check_gpu_status.remote()\n",
    "\n",
    "# Retrieve the result\n",
    "result = ray.get(future)\n",
    "\n",
    "# 4. Display results and verify GPU usage\n",
    "print(\"\\n--- GPU Task Result ---\")\n",
    "print(f\"Task executed on node: {result['hostname']}\")\n",
    "print(f\"CUDA Available (GPU Found by PyTorch): {result['cuda_available']}\")\n",
    "print(f\"CUDA Devices Found: {result['device_count']}\")\n",
    "\n",
    "# 5. Clean up the Ray connection (optional)\n",
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf31a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ba92b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ray Python 3.11.11 (VENV)",
   "language": "python",
   "name": "ray_py_3.11.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
